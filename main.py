import os
os.environ['USER_AGENT'] = 'dd'
import bs4
from langchain_community.document_loaders import WebBaseLoader # document loader to read HTML Webpages
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_ollama.llms import OllamaLLM

urls = ["https://pythonology.eu/using-pandas_ta-to-generate-technical-indicators-and-signals",]

# This is to load the documents
bs4_strainer = bs4.SoupStrainer(class_=("content-area"))
loader = WebBaseLoader(
    web_paths=(urls[0],),
    bs_kwargs={"parse_only": bs4_strainer}
)
docs = loader.load()

# this is to split the documents into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200, chunk_overlap=100, add_start_index=True
)
all_splits = text_splitter.split_documents(docs)

# this is to create the embeddings model to use
embeddings = OllamaEmbeddings(model="all-minilm:22m") # this is the model that will do the vector embeddings

# this is to create the vector database
vectordb = Chroma.from_documents(
    documents=all_splits,
    embedding=embeddings,
    collection_name="first_collection",
)

# this is to create the retriever to use the vector database
question = "What is a strategy I can easily implement to improve my trading?"
retriever = vectordb.as_retriever(search_type="similarity", 
                                  search_kwargs={"k":3}) # Kwargs stands for keyword arguments
retrieved_docs = retriever.invoke(question) # Invoke is the method to use the retriever and get a response

# this is to format the context retrieved from the vector database
context = " ".join([doc.page_content for doc in retrieved_docs])

# this is to create the LLM model to use
llm = OllamaLLM(model="gemma3:1b") # this is the model that will generate the answer
response = llm.invoke(f"""Answer the question according to the context given:
           Question: {question}.
           Context: {context}
""")
print(response) # print the answer generated by the LLM

