import os
os.environ['USER_AGENT'] = 'dd'
import bs4
from langchain_community.document_loaders import WebBaseLoader # document loader to read HTML Webpages
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_ollama.llms import OllamaLLM

urls = ["https://pythonology.eu/using-pandas_ta-to-generate-technical-indicators-and-signals",]

def web_scrape(urls):
    # This is to load the documents
    print("Loading the documents...")
    bs4_strainer = bs4.SoupStrainer(class_=("content-area"))
    loader = WebBaseLoader(
        web_paths=(urls[0],),
        bs_kwargs={"parse_only": bs4_strainer}
    )
    docs = loader.load()
    return docs

# this is to split the documents into chunks
def text_split(docs):
    print("Splitting the documents...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=120, chunk_overlap=20, add_start_index=True
    )
    all_splits = text_splitter.split_documents(docs)
    return all_splits

# this is to create the embeddings model to use
print("Creating the embeddings model...")
embeddings = OllamaEmbeddings(model="all-minilm:22m") # this is the model that will do the vector embeddings

# this is to create the vector database
def create_vectordb(all_splits, embeddings):
    print("Embedding documents and creating vectorDB...")
    vectordb = Chroma.from_documents(
        documents=all_splits,
        embedding=embeddings,
        collection_name="first_collection",
        persist_directory="./chromadb_persist",
    )
    # vectordb.persist()  # this is to save the vector database to disk
    return vectordb

# this is to create the vector database
def load_vectordb(embeddings):
    print("Loading existing vectorDB...")
    vectordb = Chroma(
        collection_name="first_collection",
        embedding_function=embeddings,
        persist_directory="./chromadb_persist",
    )
    return vectordb

# this is to run the functions to create or load the vector database
if not os.path.exists("./chromadb_persist"):
    documents = web_scrape(urls)
    all_splits = text_split(documents)
    vectordb = create_vectordb(all_splits, embeddings)
else:
    vectordb = load_vectordb(embeddings)

# this is to create the retriever to use the vector database
question = "what other tools can I use to do this other than python or any of its modules or libraries?"
retriever = vectordb.as_retriever(search_type="similarity", 
                                  search_kwargs={"k":3}) # Kwargs stands for keyword arguments
retrieved_docs = retriever.invoke(question) # Invoke is the method to use the retriever and get a response

# this is to format the context retrieved from the vector database
context = " ".join([doc.page_content for doc in retrieved_docs])

# this is to create the LLM model to use
print("Generating the answer...")
llm = OllamaLLM(model="gemma3:1b") # this is the model that will generate the answer
response = llm.invoke(f"""Answer the question according to the context given:
           Question: {question}.
           Context: {context}
""")

print(response) # print the answer generated by the LLM

